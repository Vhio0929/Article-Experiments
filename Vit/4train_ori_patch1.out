stty: 'standard input': Inappropriate ioctl for device
wandb: Currently logged in as: zhoukailong0929. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.16.4
wandb: Run data is saved locally in /root/home/zkl/vision-transformers-cifar10-main/wandb/run-20240508_062703-gzji4c65
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run vit_lr0.0001
wandb: ⭐️ View project at https://wandb.ai/zhoukailong0929/cifar10-challange
wandb: 🚀 View run at https://wandb.ai/zhoukailong0929/cifar10-challange/runs/gzji4c65
==> Preparing data..
Files already downloaded and verified
Files already downloaded and verified
==> Building model..
====现在后修改的Attention====
====现在后修改的Attention====
====现在后修改的Attention====
====现在后修改的Attention====
====现在后修改的Attention====
====现在后修改的Attention====
cuda

Epoch: 0
Traceback (most recent call last):
  File "train_cifar10.py", line 345, in <module>
    trainloss = train(epoch)
  File "train_cifar10.py", line 277, in train
    outputs = net(inputs)
  File "/data/anaconda3/envs/vit/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/anaconda3/envs/vit/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/home/zkl/vision-transformers-cifar10-main/models/vit.py", line 238, in forward
    x = self.to_patch_embedding(img)
  File "/data/anaconda3/envs/vit/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/anaconda3/envs/vit/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/anaconda3/envs/vit/lib/python3.8/site-packages/torch/nn/modules/container.py", line 217, in forward
    input = module(input)
  File "/data/anaconda3/envs/vit/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1511, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/data/anaconda3/envs/vit/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1520, in _call_impl
    return forward_call(*args, **kwargs)
  File "/data/anaconda3/envs/vit/lib/python3.8/site-packages/torch/nn/modules/linear.py", line 116, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 512.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 200.56 MiB is free. Process 1070137 has 700.00 MiB memory in use. Process 3215402 has 1.85 GiB memory in use. Process 1525329 has 19.94 GiB memory in use. Process 1525327 has 998.00 MiB memory in use. Of the allocated memory 50.16 MiB is allocated by PyTorch, and 7.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: - 0.027 MB of 0.027 MB uploadedwandb: \ 0.030 MB of 0.034 MB uploadedwandb: | 0.034 MB of 0.034 MB uploadedwandb: 🚀 View run vit_lr0.0001 at: https://wandb.ai/zhoukailong0929/cifar10-challange/runs/gzji4c65
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20240508_062703-gzji4c65/logs
